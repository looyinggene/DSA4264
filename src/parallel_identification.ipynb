{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time as time_module \n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import polyline\n",
    "from shapely.geometry import LineString, MultiLineString\n",
    "from shapely.ops import linemerge\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from geopy.distance import geodesic\n",
    "from shapely.ops import nearest_points\n",
    "import folium\n",
    "from folium import Element\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ['OGR_GEOMETRY_ACCEPT_UNCLOSED_RING'] = 'NO'\n",
    "\n",
    "# Retrieving api key\n",
    "load_dotenv(\"../key.env\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "TOKEN = os.getenv('ONEMAPTOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "geospatial_train_path = \"../datasets/geospatial_layer/TrainStation_Jul2024/RapidTransitSystemStation.shp\"\n",
    "train_stations = pd.read_excel(\"../datasets/Train_Stations.xls\")\n",
    "geospatial_train_gdf = gpd.read_file(geospatial_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get_bus_info_function.ipynb\n",
    "bus_services_df = get_bus_info(\"https://datamall2.mytransport.sg/ltaodataservice/BusServices\", api_key)\n",
    "bus_routes_df = get_bus_info(\"https://datamall2.mytransport.sg/ltaodataservice/BusRoutes\", api_key)\n",
    "bus_stops_df = get_bus_info(\"https://datamall2.mytransport.sg/ltaodataservice/BusStops\", api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "Failed to open dataset (flags=68): ../datasets/routes/filtered_bus_routes.geojson",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:130\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:134\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\\\_err.pyx:375\u001b[0m, in \u001b[0;36mfiona._err.StackChecker.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: ../datasets/routes/filtered_bus_routes.geojson: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bus_routes_gdf \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../datasets/routes/filtered_bus_routes.geojson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\geopandas\\io\\file.py:289\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\geopandas\\io\\file.py:315\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[1;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[0;32m    316\u001b[0m         crs \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;66;03m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\fiona\\env.py:457\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    454\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\fiona\\__init__.py:355\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, opener, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         path \u001b[38;5;241m=\u001b[39m _parse_path(fp)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwkt_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwkt_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    370\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[0;32m    371\u001b[0m         path,\n\u001b[0;32m    372\u001b[0m         mode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    386\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\fiona\\collection.py:226\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:876\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDriverError\u001b[0m: Failed to open dataset (flags=68): ../datasets/routes/filtered_bus_routes.geojson"
     ]
    }
   ],
   "source": [
    "bus_routes_gdf = gpd.read_file('../datasets/routes/filtered_bus_routes.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "Failed to open dataset (flags=68): ../datasets/routes/train_routes.geojson",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:130\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:134\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\\\_err.pyx:375\u001b[0m, in \u001b[0;36mfiona._err.StackChecker.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: ../datasets/routes/train_routes.geojson: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_routes_gdf_2 \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../datasets/routes/train_routes.geojson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\geopandas\\io\\file.py:289\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\geopandas\\io\\file.py:315\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[1;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[0;32m    316\u001b[0m         crs \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;66;03m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\fiona\\env.py:457\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    454\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\fiona\\__init__.py:355\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, opener, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         path \u001b[38;5;241m=\u001b[39m _parse_path(fp)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwkt_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwkt_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    370\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[0;32m    371\u001b[0m         path,\n\u001b[0;32m    372\u001b[0m         mode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    386\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\fiona\\collection.py:226\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:876\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDriverError\u001b[0m: Failed to open dataset (flags=68): ../datasets/routes/train_routes.geojson"
     ]
    }
   ],
   "source": [
    "train_routes_gdf_2 = gpd.read_file('../datasets/routes/train_routes.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_routes_stops = pd.merge(bus_routes_df, bus_stops_df, on = \"BusStopCode\", how = 'left')\n",
    "bus_routes_stops = bus_routes_stops.merge(\n",
    "    bus_services_df[['ServiceNo', 'Category']],  # Select only the columns needed for merging\n",
    "    on='ServiceNo',  # Merge on BusStopCode\n",
    "    how='left'  # Use 'left' join to keep all rows from bus_routes_stops\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and assign it back to the original DataFrame\n",
    "bus_routes_stops = bus_routes_stops.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Filter the DataFrame for rows with 'Category' equal to 'TRUNK'\n",
    "bus_routes_stops = bus_routes_stops[bus_routes_stops['Category'] == 'TRUNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alici\\anaconda3\\envs\\myenv\\Lib\\site-packages\\geopandas\\geodataframe.py:1816: FutureWarning: `unary_union` returned None due to all-None GeoSeries. In future, `unary_union` will return 'GEOMETRYCOLLECTION EMPTY' instead.\n",
      "  merged_geom = block.unary_union\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Union the geometries for the same station\n",
    "unioned_gdf = geospatial_train_gdf.dissolve(by='STN_NAM_DE',aggfunc='first')\n",
    "\n",
    "# Step 2: Calculate the centroid of the unioned polygon\n",
    "unioned_gdf['centroid'] = unioned_gdf.centroid\n",
    "unioned_gdf['geometry'] = unioned_gdf['centroid']\n",
    "\n",
    "# Reset index to clean up\n",
    "unioned_gdf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Station_Code    MRT_Station           MRT_Line TYP_CD_DES  \\\n",
      "0          NS1    Jurong East  North-South Line         MRT   \n",
      "1          NS2    Bukit Batok  North-South Line         MRT   \n",
      "2          NS3   Bukit Gombak  North-South Line         MRT   \n",
      "3          NS4  Choa Chu Kang  North-South Line         MRT   \n",
      "4          NS5        Yew Tee  North-South Line         MRT   \n",
      "\n",
      "                      geometry  \n",
      "0  POINT (17866.487 35045.184)  \n",
      "1  POINT (18676.448 36790.872)  \n",
      "2  POINT (18940.178 37860.706)  \n",
      "3  POINT (18101.056 40790.989)  \n",
      "4  POINT (18438.643 42159.628)  \n"
     ]
    }
   ],
   "source": [
    "# Function to normalize station names in train_stations_df\n",
    "def normalize_station_name(name):\n",
    "    return name.strip().upper()  # Ensure names are uppercase for consistent merging\n",
    "\n",
    "# Apply normalization function to train_stations_df\n",
    "train_stations['Normalized_Station'] = train_stations['MRT_Station'].apply(normalize_station_name)\n",
    "\n",
    "# Create a column to append \" MRT STATION\" or \" LRT STATION\" based on the MRT_Line\n",
    "train_stations['Station_MRT_LRT'] = train_stations.apply(\n",
    "    lambda row: f\"{row['Normalized_Station']} MRT STATION\" if \"LRT\" not in row['MRT_Line'] else f\"{row['Normalized_Station']} LRT STATION\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Apply normalization to geospatial_train_df\n",
    "# Strip ' MRT STATION' and ' LRT STATION' and normalize to uppercase\n",
    "unioned_gdf['Normalized_Station'] = unioned_gdf['STN_NAM_DE'].str.strip().str.upper()\n",
    "\n",
    "# Perform the merge on 'Station_MRT_LRT' from train_stations and 'Normalized_Station' from unioned_gdf\n",
    "merged_train_stations = train_stations.merge(\n",
    "    unioned_gdf,\n",
    "    how='left',\n",
    "    left_on='Station_MRT_LRT',\n",
    "    right_on='Normalized_Station'\n",
    ")\n",
    "\n",
    "# Keeping necessary columns\n",
    "columns_to_keep = ['Station_Code', 'MRT_Station', 'MRT_Line', 'TYP_CD_DES', 'geometry']\n",
    "merged_train_stations = merged_train_stations[columns_to_keep]\n",
    "\n",
    "# Check the resulting column names and sample data\n",
    "print(merged_train_stations.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Station_Code    MRT_Station           MRT_Line   Longitude  Latitude  \\\n",
      "0          NS1    Jurong East  North-South Line   103.742263  1.333209   \n",
      "1          NS2    Bukit Batok  North-South Line   103.749541  1.348997   \n",
      "2          NS3   Bukit Gombak  North-South Line   103.751910  1.358672   \n",
      "3          NS4  Choa Chu Kang  North-South Line   103.744369  1.385172   \n",
      "4          NS5        Yew Tee  North-South Line   103.747402  1.397550   \n",
      "\n",
      "  Train_Line  Station_No  \n",
      "0         NS           1  \n",
      "1         NS           2  \n",
      "2         NS           3  \n",
      "3         NS           4  \n",
      "4         NS           5  \n"
     ]
    }
   ],
   "source": [
    "#  Convert Pandas DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(merged_train_stations, geometry='geometry')\n",
    "\n",
    "#  Reproject the GeoDataFrame to EPSG:4326 (WGS 84 - latitude/longitude)\n",
    "gdf_4326 = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Extract Longitude and Latitude from the reprojected geometries\n",
    "gdf_4326['Longitude'] = gdf_4326.geometry.x\n",
    "gdf_4326['Latitude'] = gdf_4326.geometry.y\n",
    "\n",
    "#  Convert back to a Pandas DataFrame (if you don't need the geometry anymore)\n",
    "merged_train_stations = pd.DataFrame(gdf_4326)\n",
    "\n",
    "# Removing redundant columns\n",
    "columns_to_keep = ['Station_Code', 'MRT_Station', 'MRT_Line', 'Longitude', 'Latitude']\n",
    "merged_train_stations = merged_train_stations[columns_to_keep]\n",
    "merged_train_stations['Train_Line'] = merged_train_stations['Station_Code'].str.extract(r'([A-Za-z]+)')\n",
    "merged_train_stations['Station_No'] = merged_train_stations['Station_Code'].str.extract(r'(\\d+)').fillna(1).astype(int)\n",
    "print(merged_train_stations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This is the train_lines gdf created in \"./parallel_routes.ipynb\" solely by constructing line strings between mrt stations\n",
    "train_routes_gdf = gpd.read_file(\"../datasets/routes/train_lines.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bus stops (Pandas DataFrame) to GeoDataFrame with geometry points\n",
    "bus_routes_stops_gdf = gpd.GeoDataFrame(\n",
    "    bus_routes_stops,\n",
    "    geometry=gpd.points_from_xy(bus_routes_stops['Longitude'], bus_routes_stops['Latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.005 \n",
    "# Custom function to merge dictionaries\n",
    "def merge_dicts(dicts):\n",
    "    merged = defaultdict(list)\n",
    "    for d in dicts:\n",
    "        for key, value in d.items():\n",
    "            merged[key].extend(value if isinstance(value, list) else [value])\n",
    "    return dict(merged)\n",
    "\n",
    "# Function to calculate parallelness and aggregate by ServiceNo across directions\n",
    "def calculate_parallelness_with_aggregation(bus_routes_stops_gdf, bus_routes_gdf, train_routes_gdf):\n",
    "    parallel_scores = []\n",
    "\n",
    "    for _, bus_route in bus_routes_gdf.iterrows():\n",
    "        service_no = bus_route['index'].split('_')[0]\n",
    "        direction = int(bus_route['index'].split('_')[-1].replace('direction_', ''))\n",
    "        \n",
    "        # Filter stops for the current service and direction\n",
    "        route_stops = bus_routes_stops_gdf[(bus_routes_stops_gdf['ServiceNo'] == service_no) & \n",
    "                                           (bus_routes_stops_gdf['Direction'] == direction)]\n",
    "        total_stops = len(route_stops)\n",
    "\n",
    "        # Skip if no stops for this direction\n",
    "        if total_stops == 0:\n",
    "            continue\n",
    "\n",
    "        # Dictionary to store parallel stop details per train line\n",
    "        train_line_parallel_data = {}\n",
    "        total_softmax_score = 0  # Aggregate softmax score for all train lines\n",
    "\n",
    "        # Process each MRT line and calculate softmax-based parallelness\n",
    "        for mrt_line_id, mrt_line_geom in train_routes_gdf.set_index('Train_Line').geometry.items():\n",
    "            softmax_scores = []\n",
    "            parallel_stop_codes = []\n",
    "            parallel_stop_coords = []\n",
    "\n",
    "            for _, stop in route_stops.iterrows():\n",
    "                bus_stop_geom = stop.geometry\n",
    "                bus_stop_code = stop['BusStopCode']\n",
    "\n",
    "                # Calculate distance from bus stop to MRT line\n",
    "                distance = bus_stop_geom.distance(mrt_line_geom)\n",
    "\n",
    "                # Calculate softmax-based score\n",
    "                softmax_score = np.exp(-ALPHA * distance)\n",
    "\n",
    "                # Consider it as parallel if the softmax score is significant (e.g., above 0.5)\n",
    "                if softmax_score > 0.5:\n",
    "                    softmax_scores.append(softmax_score)\n",
    "                    parallel_stop_codes.append(bus_stop_code)\n",
    "                    parallel_stop_coords.append((bus_stop_geom.x, bus_stop_geom.y))\n",
    "\n",
    "            # Sum softmax scores for the current train line\n",
    "            line_softmax_score = sum(softmax_scores)\n",
    "            total_softmax_score += line_softmax_score\n",
    "\n",
    "            # Store parallel stop details if any parallel stops found\n",
    "            if parallel_stop_codes:\n",
    "                train_line_parallel_data[mrt_line_id] = {\n",
    "                    'ParallelStopCodes': parallel_stop_codes,\n",
    "                    'ParallelStopCoordinates': parallel_stop_coords,\n",
    "                    'LineSoftmaxScore': line_softmax_score\n",
    "                }\n",
    "\n",
    "        # Calculate normalized parallelness score across all lines\n",
    "        normalized_parallelness = total_softmax_score / total_stops if total_stops > 0 else 0\n",
    "\n",
    "        # Append the result for the current bus service and direction\n",
    "        parallel_scores.append({\n",
    "            'ServiceNo': service_no,\n",
    "            'Direction': direction,\n",
    "            'TotalStops': total_stops,\n",
    "            'CombinedParallelnessScore': normalized_parallelness,\n",
    "            'TrainLineParallelData': train_line_parallel_data  # Stores details for each train line\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    parallel_results_df = pd.DataFrame(parallel_scores)\n",
    "\n",
    "    # Step 2: Aggregate by ServiceNo across both directions\n",
    "    aggregated_results = (\n",
    "        parallel_results_df.groupby('ServiceNo')\n",
    "        .agg({\n",
    "            'TotalStops': 'sum',\n",
    "            'CombinedParallelnessScore': lambda x: np.average(x, weights=parallel_results_df.loc[x.index, 'TotalStops']),\n",
    "            'TrainLineParallelData': merge_dicts  # Use custom function to merge dictionary data\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort by CombinedParallelnessScore for top results\n",
    "    aggregated_results = aggregated_results.sort_values(by='CombinedParallelnessScore', ascending=False)\n",
    "\n",
    "    return aggregated_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now proceed with the parallelness calculation function\n",
    "final_results_1 = calculate_parallelness_with_aggregation(bus_routes_stops_gdf, bus_routes_gdf, train_routes_gdf)\n",
    "\n",
    "# Display the top-ranked results\n",
    "print(final_results_1[['ServiceNo', 'TotalStops', 'CombinedParallelnessScore', 'TrainLineParallelData']].head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Incorporating Angle Difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DISTANCE_THRESHOLD = 350  # Distance in meters\n",
    "ANGLE_THRESHOLD = 25  # Maximum angle in degrees for parallelness\n",
    "CONSECUTIVE_WEIGHT = 1.2  # Weight multiplier for consecutive parallel stops\n",
    "\n",
    "# Custom function to merge dictionaries\n",
    "def merge_dicts(dicts):\n",
    "    merged = defaultdict(list)\n",
    "    for d in dicts:\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                merged[key].append(value)\n",
    "            else:\n",
    "                merged[key].extend(value if isinstance(value, list) else [value])\n",
    "    return dict(merged)\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "import math\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Function to calculate angle between two points\n",
    "def calculate_angle(bus_p1, bus_p2, train_p1, train_p2):\n",
    "    bus_dx = bus_p1.x - bus_p2.x\n",
    "    bus_dy = bus_p1.y - bus_p2.y\n",
    "    bus_norm = math.sqrt(bus_dy ** 2 + bus_dx ** 2)\n",
    "    \n",
    "    train_dx = train_p1.x - train_p2.x\n",
    "    train_dy = train_p1.y - train_p2.y\n",
    "    train_norm = math.sqrt(train_dy ** 2 + train_dx ** 2)\n",
    "\n",
    "    if bus_norm == 0 or train_norm == 0:\n",
    "        return 0\n",
    "\n",
    "    norm_dot_prod = (bus_dx * train_dx + bus_dy * train_dy) / (bus_norm * train_norm)\n",
    "    angle = np.degrees(np.arccos(norm_dot_prod))\n",
    "\n",
    "    if angle >= 90:\n",
    "        angle = 180 - angle\n",
    "\n",
    "    return angle\n",
    "\n",
    "# Updated function to calculate parallelness with consecutive parallel segments and combine directions\n",
    "def calculate_parallelness_combined_directions(bus_routes_stops_gdf, train_routes_gdf):\n",
    "    parallel_scores = []\n",
    "\n",
    "    # Group by ServiceNo and Direction to process each bus service route individually\n",
    "    for service_no, service_data in bus_routes_stops_gdf.groupby('ServiceNo'):\n",
    "        directions = service_data['Direction'].unique()\n",
    "\n",
    "        # Process each direction\n",
    "        for direction in directions:\n",
    "            route_stops = service_data[service_data['Direction'] == direction]\n",
    "            total_stops = len(route_stops)\n",
    "\n",
    "            if total_stops < 2:\n",
    "                continue\n",
    "\n",
    "            train_line_parallel_data = {}\n",
    "            total_parallel_count = 0\n",
    "            consecutive_parallel_count = 0  # Tracks consecutive parallel segments\n",
    "\n",
    "            # Process each MRT line to calculate both distance-based parallelness and angular similarity\n",
    "            for mrt_line_id, mrt_line_geom in train_routes_gdf.set_index('Train_Line').geometry.items():\n",
    "                parallel_stop_codes = []\n",
    "                parallel_stop_coords = []\n",
    "                line_parallel_count = 0  # Track count for this MRT line\n",
    "                consecutive_segment_length = 0  # Length of the current consecutive parallel sequence\n",
    "\n",
    "                for i in range(total_stops - 1):\n",
    "                    bus_stop_geom1 = route_stops.iloc[i].geometry\n",
    "                    bus_stop_geom2 = route_stops.iloc[i + 1].geometry\n",
    "                    bus_stop_code = route_stops.iloc[i]['BusStopCode']\n",
    "\n",
    "                    bus_segment = LineString([bus_stop_geom1, bus_stop_geom2])\n",
    "                    segment_midpoint = bus_segment.interpolate(0.5, normalized=True)\n",
    "\n",
    "                    # Distance and angle calculations\n",
    "                    nearest_train_segment = mrt_line_geom.interpolate(mrt_line_geom.project(segment_midpoint))\n",
    "                    nearest_train_point = mrt_line_geom.interpolate(mrt_line_geom.project(nearest_train_segment) + 0.01)\n",
    "                    distance = geodesic((segment_midpoint.y, segment_midpoint.x), (nearest_train_segment.y, nearest_train_segment.x)).meters\n",
    "                    angle_difference = calculate_angle(bus_stop_geom1, bus_stop_geom2, nearest_train_segment, nearest_train_point)\n",
    "\n",
    "                    # Check parallel conditions and manage consecutive parallel segments\n",
    "                    if distance <= DISTANCE_THRESHOLD and angle_difference <= ANGLE_THRESHOLD:\n",
    "                        line_parallel_count += 1\n",
    "                        parallel_stop_codes.append(bus_stop_code)\n",
    "                        parallel_stop_coords.append((bus_stop_geom1.x, bus_stop_geom1.y))\n",
    "\n",
    "                        # Track consecutive segments\n",
    "                        consecutive_segment_length += 1\n",
    "                    else:\n",
    "                        # Apply weight if there was a consecutive parallel segment\n",
    "                        if consecutive_segment_length > 1:\n",
    "                            consecutive_parallel_count += consecutive_segment_length * CONSECUTIVE_WEIGHT\n",
    "                        consecutive_segment_length = 0\n",
    "\n",
    "                # Finalize consecutive parallel count for the last sequence\n",
    "                if consecutive_segment_length > 1:\n",
    "                    consecutive_parallel_count += consecutive_segment_length * CONSECUTIVE_WEIGHT\n",
    "\n",
    "                # Store parallel stop details if any parallel stops found for this MRT line\n",
    "                if parallel_stop_codes:\n",
    "                    train_line_parallel_data[mrt_line_id] = {\n",
    "                        'ParallelStopCodes': parallel_stop_codes,\n",
    "                        'ParallelStopCoordinates': parallel_stop_coords,\n",
    "                        'ParallelCount': line_parallel_count\n",
    "                    }\n",
    "                    total_parallel_count += line_parallel_count\n",
    "\n",
    "            # Calculate a weighted score considering consecutive segments\n",
    "            weighted_parallel_score = (total_parallel_count + consecutive_parallel_count) / total_stops if total_stops > 0 else 0\n",
    "\n",
    "            parallel_scores.append({\n",
    "                'ServiceNo': service_no,\n",
    "                'Direction': direction,\n",
    "                'TotalStops': total_stops,\n",
    "                'WeightedParallelScore': weighted_parallel_score,\n",
    "                'TrainLineParallelData': train_line_parallel_data\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    parallel_results_df = pd.DataFrame(parallel_scores)\n",
    "\n",
    "    # Step 2: Aggregate by ServiceNo to combine directions\n",
    "    aggregated_results = (\n",
    "        parallel_results_df.groupby('ServiceNo')\n",
    "        .agg({\n",
    "            'TotalStops': 'sum',\n",
    "            'WeightedParallelScore': 'mean',  # Average score across directions\n",
    "            'TrainLineParallelData': merge_dicts  # Merge parallel data across directions\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort by WeightedParallelScore for top results\n",
    "    aggregated_results = aggregated_results.sort_values(by='WeightedParallelScore', ascending=False)\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "# Run the function and display the top-ranked results aggregated by ServiceNo\n",
    "final_results = calculate_parallelness_combined_directions(bus_routes_stops_gdf, train_routes_gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_results.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the final results to exclude ServiceNos with alphabetic characters\n",
    "filtered_final_results = final_results[~final_results['ServiceNo'].str.contains(r'[A-Za-z]')]\n",
    "\n",
    "# Display filtered results\n",
    "print(filtered_final_results[['ServiceNo', 'TotalStops', 'WeightedParallelScore', 'TrainLineParallelData']].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = final_results[final_results['ServiceNo'] == '36']['TrainLineParallelData'].iloc[0]\n",
    "pprint(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_final_results.to_csv('../datasets/filtered_final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_df = pd.read_csv('../datasets/filtered_final_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting top 5 bus service routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_geo_data = gpd.read_file('../datasets/routes/routes.min.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_routes_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 111 direction 2\n",
      "No data for 36 direction 2\n"
     ]
    }
   ],
   "source": [
    "# Get the top 3 bus routes by some criteria (e.g., WeightedParallelScore)\n",
    "top_3_bus_routes = bus_geo_data[bus_geo_data['number'].isin(['67', '111', '36'])]\n",
    "\n",
    "# Colors for the top 3 bus services and their directions\n",
    "bus_colors = [\n",
    "    ('pink', 'lightcoral'),  # Service 1: Direction 1, Direction 2\n",
    "    ('cyan', 'lightblue'),     # Service 2: Direction 1, Direction 2\n",
    "    ('purple', 'plum')         # Service 3: Direction 1, Direction 2\n",
    "]\n",
    "\n",
    "# Define color mapping for each MRT line\n",
    "color_map = {\n",
    "    'TE': 'brown',\n",
    "    'EW': 'green',\n",
    "    'CC': '#FFA500',  # Yellowish orange\n",
    "    'NE': 'purple',\n",
    "    'NS': 'red',\n",
    "    'DT': 'blue',\n",
    "    'SK': 'grey',\n",
    "    'BP': 'grey',\n",
    "    'PG': 'grey',\n",
    "    'CE': 'orange',\n",
    "    'CG' : 'grey'\n",
    "}\n",
    "\n",
    "mrt_map = {\n",
    "    'TE': 'Thomson East-Coast Line',\n",
    "    'EW': 'East-West Line',\n",
    "    'CC': 'Circle Line',  # Yellowish orange\n",
    "    'NE': 'North-East Line',\n",
    "    'NS': 'North-South Line',\n",
    "    'DT': 'Downtown Line',\n",
    "    'SK': 'SENG KANG LRT',\n",
    "    'BP': 'BUKIT PANJANG LRT',\n",
    "    'PG': 'PUNGGOL LRT',\n",
    "    'CE': 'Circle Line Extension',\n",
    "    'CG' : 'Changi Airport Branch'\n",
    "}\n",
    "\n",
    "\n",
    "# Function to apply an offset to the coordinates of a LineString\n",
    "def offset_line(line, offset=0.0001):\n",
    "    \"\"\"\n",
    "    Applies an offset to the coordinates of a LineString.\n",
    "    Args:\n",
    "        line (LineString): The original LineString.\n",
    "        offset (float): The offset value to apply to the coordinates.\n",
    "    Returns:\n",
    "        LineString: The new LineString with applied offset.\n",
    "    \"\"\"\n",
    "    new_coords = [(x + offset, y + offset) for x, y in line.coords]\n",
    "    return LineString(new_coords)\n",
    "\n",
    "\n",
    "# Function to create a legend\n",
    "def add_mrt_legend(map_object, color_map, mrt_map, bus_number):\n",
    "    legend_html = f'''\n",
    "     <div style=\"\n",
    "     position: fixed;\n",
    "     bottom: 50px;\n",
    "     left: 50px;\n",
    "     width: 220px;\n",
    "     height: auto;\n",
    "     z-index:9999;\n",
    "     font-size:14px;\n",
    "     background-color: white;\n",
    "     padding: 10px;\n",
    "     border: 2px solid black;\n",
    "     \">\n",
    "     <b>Bus Service {bus_number}</b><br><br>\n",
    "     <b>Legend for MRT Lines:</b><br><br>\n",
    "    '''\n",
    "    for line_code, color in color_map.items():\n",
    "        if line_code in mrt_map:  # Only add MRT lines that are in mrt_map\n",
    "            line_name = mrt_map[line_code]\n",
    "            legend_html += f'<i style=\"background:{color}; width: 10px; height: 10px; display: inline-block; margin-right: 5px;\"></i> {line_name}<br>'\n",
    "    \n",
    "    legend_html += '</div>'\n",
    "    \n",
    "    map_object.get_root().html.add_child(Element(legend_html))\n",
    "\n",
    "# Create three separate maps for each bus service\n",
    "for i, bus_number in enumerate(['67', '111', '36']):\n",
    "    # Create a base Folium map centered around Singapore\n",
    "    sg_map = folium.Map(location=[1.3521, 103.8198], zoom_start=12, tiles='CartoDB positron')\n",
    "    \n",
    "    # Plot all train lines in grey\n",
    "    for _, row in train_routes_gdf.iterrows():\n",
    "        if isinstance(row['geometry'], LineString):\n",
    "            train_line = row['Train_Line']\n",
    "            long_name = mrt_map.get(train_line, 'Unknown Line')\n",
    "            color = color_map.get(train_line, 'black')  # Default to 'black' if not found in the color map\n",
    "            coords = [(point[1], point[0]) for point in row['geometry'].coords]  # Switch (lon, lat) to (lat, lon)\n",
    "            folium.PolyLine(\n",
    "                locations=coords,\n",
    "                color=color,\n",
    "                weight=2,\n",
    "                opacity=0.5,\n",
    "                popup=f\"{long_name}\"\n",
    "            ).add_to(sg_map)\n",
    "    # Add MRT stations as CircleMarkers\n",
    "    for _, station_row in merged_train_stations.iterrows():\n",
    "        line_code = station_row['Station_Code'][:2]  # Adjust this as needed based on your data structure\n",
    "        station_color = color_map.get(line_code, 'grey')\n",
    "        folium.CircleMarker(\n",
    "            location=(station_row['Latitude'], station_row['Longitude']),\n",
    "            radius=4,\n",
    "            color=station_color,\n",
    "            fill=True,\n",
    "            fill_color=station_color,\n",
    "            fill_opacity=0.7,\n",
    "            popup=f\"{station_row['Station_Code']}<br>{station_row['MRT_Station']}\",\n",
    "            tooltip=f\"{station_row['Station_Code']} - {station_row['MRT_Station']}\"\n",
    "        ).add_to(sg_map)\n",
    "\n",
    "    # Filter the routes for the current bus number\n",
    "    bus_routes = top_3_bus_routes[top_3_bus_routes['number'] == bus_number]\n",
    "    \n",
    "    # Plot the bus stops associated with the current bus number\n",
    "    bus_stops = bus_routes_stops_gdf[bus_routes_stops_gdf['ServiceNo'] == bus_number]\n",
    "    for _, stop_row in bus_stops.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=(stop_row['Latitude'], stop_row['Longitude']),\n",
    "            radius=3,  # Adjust the radius as needed for better visualization\n",
    "            color='black',  # Color of the circle's outline\n",
    "            fill=True,\n",
    "            fill_color='black',  # Color of the filled circle\n",
    "            fill_opacity=0.7,\n",
    "            popup=f\"{stop_row['BusStopCode']}<br>{stop_row['Description']}\",\n",
    "            tooltip=f\"{stop_row['BusStopCode']} - {stop_row['Description']}\"\n",
    "        ).add_to(sg_map)\n",
    "\n",
    "    # Check if both directions exist\n",
    "    if not bus_routes[bus_routes['pattern'] == 0].empty:\n",
    "        bus_line_1 = bus_routes[bus_routes['pattern'] == 0]['geometry'].values[0]\n",
    "        service_name_1 = f\"{bus_number}_direction_1\"\n",
    "        color_1 = bus_colors[i][0]\n",
    "\n",
    "        # Offset and plot direction 1\n",
    "        offset_line_1 = offset_line(bus_line_1, offset=0.0001)\n",
    "        folium.PolyLine(\n",
    "            locations=[(lat, lon) for lon, lat in offset_line_1.coords],\n",
    "            color=color_1,\n",
    "            weight=5,\n",
    "            opacity=0.8,\n",
    "            tooltip=service_name_1\n",
    "        ).add_to(sg_map)\n",
    "    else:\n",
    "        print(f\"No data for {bus_number} direction 1\")\n",
    "\n",
    "    if not bus_routes[bus_routes['pattern'] == 1].empty:\n",
    "        bus_line_2 = bus_routes[bus_routes['pattern'] == 1]['geometry'].values[0]\n",
    "        service_name_2 = f\"{bus_number}_direction_2\"\n",
    "        color_2 = bus_colors[i][1]\n",
    "\n",
    "        # Offset and plot direction 2\n",
    "        offset_line_2 = offset_line(bus_line_2, offset=-0.0001)\n",
    "        folium.PolyLine(\n",
    "            locations=[(lat, lon) for lon, lat in offset_line_2.coords],\n",
    "            color=color_2,\n",
    "            weight=5,\n",
    "            opacity=0.8,\n",
    "            tooltip=service_name_2\n",
    "        ).add_to(sg_map)\n",
    "    else:\n",
    "        print(f\"No data for {bus_number} direction 2\")\n",
    "\n",
    "    add_mrt_legend(sg_map, color_map, mrt_map, bus_number)\n",
    "    sg_map.save(f'../datasets/routes/map_bus_service_{i + 1}.html')\n",
    "    sg_map\n",
    "\n",
    "# The maps will be saved as HTML files: map_bus_service_1.html, map_bus_service_2.html, map_bus_service_3.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA4264",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
